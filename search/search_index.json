{"config":{"lang":["fr"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Laboratoire d\u2019apprentissage","text":"<p>Je me pr\u00e9sente, Benjamin, 28 Ans et Administrateur Syst\u00e8me dans un groupe d'assurance. Je mets \u00e0 disposition ce wiki pour les personnes qui comme moi, cherchent \u00e0 en apprendre toujours plus sur les diff\u00e9rentes technologies.</p> <p>Cr\u00e9ation d\u2019un laboratoire d\u2019apprentissage pour essayer de nouvelles technologies d\u2019infrastructure, d\u00e9velopper les connaissances et apprendre de nouvelles choses.</p>"},{"location":"#contexte","title":"Contexte","text":"<p>Ici, pas de contexte Cloud pour le moment, l\u2019id\u00e9e est d\u2019apprendre un maximum de chose sur un contexte dit \u201cOnPremise\u201d, en local sur une ou plusieurs machines physiques. L'id\u00e9e g\u00e9n\u00e9rale est d'apprendre sur le mat\u00e9riel que l'on poss\u00e8de et d'\u00e9largir quand on le peut et quand on le souhaite.</p> <p>Il est bon de savoir que bien des choses sont r\u00e9alisables avec un ordinateur, un peu de m\u00e9moire vive, quelques processeurs et de la volont\u00e9 !</p>"},{"location":"#technologies-principales","title":"Technologies principales","text":"<p>Voici les diff\u00e9rentes technologies qui seront abord\u00e9es :</p> <ul> <li>Automatisation</li> <li>Virtualisation</li> <li>Conteneur</li> <li>Sauvegarde</li> <li>Stockage</li> <li>R\u00e9seau</li> <li>Pratique DevOps/GitOps</li> </ul> <p>Et bien d'autre chose !</p>"},{"location":"automation/","title":"Bienvenue !","text":""},{"location":"automation/#information","title":"Information","text":"<p>Pas grand chose pour le moment !</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"kubernetes/","title":"Kubernetes","text":"<p>Dans ce chapitre, nous verrons comment manipuler et pratique sur Kubernetes, dans un premier temps au travers de minikube mais aussi au travers d'un Cluste Kubernetes en VM.</p>"},{"location":"kubernetes/#pourquoi-kubernetes","title":"Pourquoi Kubernetes ?","text":"<p>Kubernetes, souvent abr\u00e9g\u00e9 en \"K8s\", est une plateforme open-source con\u00e7ue pour automatiser le d\u00e9ploiement, la mise \u00e0 l'\u00e9chelle et la gestion des applications conteneuris\u00e9es. Voici les principales raisons pour lesquelles Kubernetes est largement adopt\u00e9 :</p> <ol> <li>Automatisation du D\u00e9ploiement et de la Gestion</li> <li>Portabilit\u00e9 et Flexibilit\u00e9</li> <li>Gestion de la Mise \u00e0 l'\u00c9chelle</li> <li>Haute Disponibilit\u00e9 et R\u00e9partition de Charge</li> <li>Gestion de la Configuration et des Secrets</li> <li>\u00c9cosyst\u00e8me et Communaut\u00e9 \u00c9largis</li> <li>Support pour les D\u00e9ploiements Complexes</li> <li>R\u00e9cup\u00e9ration Automatique et Self-Healing</li> <li>Gestion des Ressources</li> </ol> <p>Kubernetes est adopt\u00e9 pour sa capacit\u00e9 \u00e0 simplifier et automatiser la gestion des applications conteneuris\u00e9es, \u00e0 offrir une portabilit\u00e9 et une flexibilit\u00e9 accrues, et \u00e0 garantir la haute disponibilit\u00e9 et la mise \u00e0 l'\u00e9chelle des applications. Son \u00e9cosyst\u00e8me riche et sa communaut\u00e9 active en font une solution robuste et \u00e9volutive pour les environnements de production modernes.</p>"},{"location":"kubernetes/k8s-internal-components/","title":"Kubernetes, briques internes","text":"<p>Nous allons voir au travers de minikube les diff\u00e9rentes briques internes importantes, sachant que nous sommes sous Minikube, certaines briques sont diff\u00e9rentes d'un Cluster K8S classique.</p> <p>Vous trouverez la documentation officielle sur les briques ici : Composants de Kubernetes</p> <p>Voici un sch\u00e9ma de principe des briques internes de Kube :</p> <p></p>","tags":["Kubernetes"]},{"location":"kubernetes/k8s-internal-components/#espace-de-nom-kube-system","title":"Espace de nom kube-system","text":"<p>Un namespace permet de scinder des \u00e9lements \u00e0 l'int\u00e9rieur du Cluster, exemple avec le namespace kube-system, essentiel \u00e0 tout cluster Kube.</p> <p>Sur notre environnement, voici ce qu'il contient : </p> <pre><code>k get pods -n kube-system\n\nNAME                               READY   STATUS    RESTARTS       AGE\ncoredns-7db6d8ff4d-5jv58           1/1     Running   4 (11m ago)    30d\ncoredns-7db6d8ff4d-mrggh           1/1     Running   4 (11m ago)    30d\netcd-minikube                      1/1     Running   4 (11m ago)    30d\nkindnet-4nbpw                      1/1     Running   4 (11m ago)    30d\nkube-apiserver-minikube            1/1     Running   10 (10m ago)   30d\nkube-controller-manager-minikube   1/1     Running   10 (10m ago)   30d\nkube-proxy-lk8gl                   1/1     Running   4 (11m ago)    30d\nkube-scheduler-minikube            1/1     Running   4 (11m ago)    30d\nkube-vip-minikube                  1/1     Running   3 (11m ago)    28d\nmetrics-server-c59844bb4-9fxrl     1/1     Running   0              8m9s\nstorage-provisioner                1/1     Running   6 (11m ago)    30d\n</code></pre>","tags":["Kubernetes"]},{"location":"kubernetes/k8s-internal-components/#composants-internes-essentiels","title":"Composants internes essentiels","text":"","tags":["Kubernetes"]},{"location":"kubernetes/k8s-internal-components/#coredns","title":"CoreDNS","text":"<p>CoreDNS est le composant qui g\u00e8re en interne toute la partie r\u00e9solution de nom au sein du Cluster Kube.</p> <p>Pour connaitre la configuration initiale de CoreDNS, il suffit de regarder la ConfigMap associ\u00e9e :</p> <pre><code>apiVersion: v1\ndata:\n  Corefile: |\n    .:53 {\n        log\n        errors\n        health {\n           lameduck 5s\n        }\n        ready\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n           pods insecure\n           fallthrough in-addr.arpa ip6.arpa\n           ttl 30\n        }\n        prometheus :9153\n        hosts {\n           192.168.59.1 host.minikube.internal\n           fallthrough\n        }\n        forward . /etc/resolv.conf {\n           max_concurrent 1000\n        }\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2024-06-05T20:56:33Z\"\n  name: coredns\n  namespace: kube-system\n  resourceVersion: \"320\"\n  uid: 6e061255-abd1-4d91-a86b-bac04bf0fe29\n</code></pre> <p>CoreDNS est puissant par sa flexibilit\u00e9 li\u00e9 aux plugins.</p>","tags":["Kubernetes"]},{"location":"kubernetes/k8s-internal-components/#etcd","title":"etcd","text":"<p>Etcd est une base interne NoSQL qui permet de g\u00e9rer des cl\u00e9s/valeurs. C'est cette base qui va servir au Cluster pour stocker les \u00e9lements du Cluster Kubernetes.</p> <p>En autre, c'est un composant critique d'un Cluster.</p>","tags":["Kubernetes"]},{"location":"kubernetes/k8s-internal-components/#kindnet","title":"Kindnet","text":"<p>Kindnet est utilis\u00e9 principalement dans Minikube pour la gestion du r\u00e9seau aupr\u00e8s des pods, c'est le pilote CNI (Container Network Interface).</p> <p>Ce composant est donc li\u00e9 \u00e0 minikube, il n'est pas utilis\u00e9 lors de la cr\u00e9ation d'un cluster K8S.</p>","tags":["Kubernetes"]},{"location":"kubernetes/k8s-internal-components/#apiserver","title":"APIServer","text":"<p>Le role du serveu d'API est d'accueillir toutes les requ\u00eates au sein d'un Cluster Kube (ou minikube dans notre cas), notament lors de la cr\u00e9ation des diff\u00e9rents objets tels que les namespaces, pods, d\u00e9ploiements, le stockage etc...</p> <p>Dans notre cas, en mode SingleNode, le pod n'est d\u00e9marr\u00e9 qu'une seule fois sur le noeud lab\u00e9lis\u00e9 master/control-plane. Plus le cluster contient ce type de noeud, plus il y aura un scaling horizontal de ce type de pod afin d'\u00e9largir la r\u00e9silience.</p>","tags":["Kubernetes"]},{"location":"kubernetes/k8s-internal-components/#kube-proxy","title":"Kube Proxy","text":"<p>Le proxy kube est un pod qui est d\u00e9marr\u00e9 sur chacun des noeuds d'un Cluster, il permet de faire la liaison r\u00e9seau au niveau des services kubernetes (svc)</p> <p>Dans les grandes lignes :</p> <ul> <li>Redirection du trafic</li> <li>Equilibrage de charge</li> <li>Support des protocoles standards, TCP/UDP/SCTP</li> <li>Mise \u00e0 jour dynamique (en terme de modification des svc)</li> </ul> <p>Plusieurs modes de fonctionnement :</p> <ol> <li>Userspace Mode : Redirige le trafic en passant par l'espace utilisateur. Ce mode est moins performant et est rarement utilis\u00e9.</li> <li>iptables Mode : Utilise iptables pour rediriger le trafic au niveau du noyau Linux. C'est le mode le plus commun et offre de bonnes performances.</li> <li>IPVS Mode : Utilise IP Virtual Server (IPVS) pour des performances encore meilleures que celles d'iptables, particuli\u00e8rement adapt\u00e9 pour des environnements \u00e0 grande \u00e9chelle.</li> </ol>","tags":["Kubernetes"]},{"location":"kubernetes/k8s-internal-components/#scheduler","title":"Scheduler","text":"<p>Cette brique permet de d\u00e9finir o\u00f9 les pods seront positionn\u00e9s en fonction de la configuration demand\u00e9 par l'utilisateur au sein d'un d\u00e9ploiement, replicatset ou bien d'un daemon-set. </p>","tags":["Kubernetes"]},{"location":"kubernetes/k8s-internal-components/#controller-manager","title":"Controller Manager","text":"<p>C'est le composant qui permet d'\u00e9tablir un suivi permanent des composants du syst\u00e8me.</p>","tags":["Kubernetes"]},{"location":"kubernetes/k8s-internal-components/#kubelet","title":"Kubelet","text":"<p>Kubelet est un agent d\u00e9marr\u00e9 sur chaqune machine d'un Cluster qui permet de g\u00e9rer certain composant Kubernetes, ils peuvent \u00eatre sous le format d'un service systemd (daemon) ou en conteneur classique.</p> <p>Lors du d\u00e9marrage d'un noeud, Kubelet peut d\u00e9marrer des pods grace \u00e0 des fichiers de configuration yaml se trouvant sur le noeud.</p> <p>Exemple avec minikube : <pre><code>minikube ssh\n                         _             _            \n            _         _ ( )           ( )           \n  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __  \n/' _ ` _ `\\| |/' _ `\\| || , &lt;  ( ) ( )| '_`\\  /'__`\\\n| ( ) ( ) || || ( ) || || |\\`\\ | (_) || |_) )(  ___/\n(_) (_) (_)(_)(_) (_)(_)(_) (_)`\\___/'(_,__/'`\\____)\n\n$ cd /etc/kubernetes/manifests/\n$ ls -l\ntotal 16\n-rw------- 1 root root 2506 Jul  6 17:33 etcd.yaml\n-rw------- 1 root root 3665 Jul  6 17:33 kube-apiserver.yaml\n-rw------- 1 root root 2974 Jul  6 17:33 kube-controller-manager.yaml\n-rw------- 1 root root 1464 Jul  6 17:33 kube-scheduler.yaml\n</code></pre></p> <p>Pour finir, Kubelet ne g\u00e8re que les pods cr\u00e9\u00e9s par Kubernetes, en autre, ceux utiles au Cluster.</p>","tags":["Kubernetes"]},{"location":"kubernetes/helm/","title":"Introduction \u00e0 Helm","text":"<p>Helm a \u00e9t\u00e9 introduit par la communaut\u00e9 afin de r\u00e9pondre \u00e0 un besoin de simplification de l'installation des applications dans un cluster Kubernetes. Avant l'arriv\u00e9e de Helm, les administrateurs devaient g\u00e9rer le cycle de vie des fichiers de configuration YAML, leur personnalisation ainsi que l'int\u00e9gration des produits eux m\u00eames.</p> <p>Un cluster Kubernetes fonctionne \u00e0 l'aide de nombreux produits (Elasticsearch, Kafka, Prometheus, Grafana, Nginx, etc.). Les ma\u00eetriser tous peut \u00eatre chronophage.</p> <p>Les charts Helm sont une aide pr\u00e9cieuse pour s'affranchir des probl\u00e9matiques d'installaton initiale.</p> <p>Sources :</p> <ul> <li>Helm</li> <li>Livre Expert Kubernetes par Yanning Perr\u00e9</li> </ul>"},{"location":"kubernetes/helm/#principe-de-fonctionnement","title":"Principe de fonctionnement","text":"<p>En version 3, Helm fonctionne \u00e0 l'aide d'un client \u00e9crit en Go, se dernier d'appuie sur l'API de Kubernetes . Le programme se pr\u00e9sente sous forme de binaire ind\u00e9pendant, comme l'est kubectl par exemple. Il ne n\u00e9cessite pas de d\u00e9pendance particuli\u00e8re.</p>"},{"location":"kubernetes/helm/getting-started-helm/","title":"En Avant !","text":"","tags":["Kubernetes","Minikube","Helm"]},{"location":"kubernetes/helm/getting-started-helm/#installation-sous-linux","title":"Installation sous Linux","text":"<p>Pour l'installer sur un syst\u00e8me Linux, rien de plus simple :</p> <pre><code>$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\n$ chmod 700 get_helm.sh\n$ ./get_helm.sh\n</code></pre> <p>Helm s'appuie sur la configuration du contexte actuelle pour la connexion \u00e0 un cluster Kubernetes.</p> <p>Cependant, vous pouvez librement d\u00e9finir des variables d'environnements afin de changer de contexte : <pre><code># helm --help\n...\n| $KUBECONFIG                        | set an alternative Kubernetes configuration file (default \"~/.kube/config\")                                |\n| $HELM_KUBEAPISERVER                | set the Kubernetes API Server Endpoint for authentication                                                  |\n| $HELM_KUBECAFILE                   | set the Kubernetes certificate authority file.                                                             |\n| $HELM_KUBEASGROUPS                 | set the Groups to use for impersonation using a comma-separated list.                                      |\n| $HELM_KUBEASUSER                   | set the Username to impersonate for the operation.                                                         |\n| $HELM_KUBECONTEXT                  | set the name of the kubeconfig context.                                                                    |\n| $HELM_KUBETOKEN                    | set the Bearer KubeToken used for authentication.                                                          |\n| $HELM_KUBEINSECURE_SKIP_TLS_VERIFY | indicate if the Kubernetes API server's certificate validation should be skipped (insecure)                |\n| $HELM_KUBETLS_SERVER_NAME          | set the server name used to validate the Kubernetes API server certificate                                 |\n...\n</code></pre></p>","tags":["Kubernetes","Minikube","Helm"]},{"location":"kubernetes/helm/getting-started-helm/#ajout-dune-source-helm","title":"Ajout d'une source Helm","text":"<p>Dans la version 3 de Helm, il n'y a plus de source configur\u00e9e par d\u00e9faut, ce qui veut dire que nous ne pouvons pas effectuer de recherche pour une chart sp\u00e9cifique.</p> <p>Nous allons ajouter la source du d\u00e9p\u00f4t Bitnami</p> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\n\n\"bitnami\" has been added to your repositories\n</code></pre> <p>On peut mettre \u00e0 jour le d\u00e9p\u00f4t via : <code>helm repo update</code></p>","tags":["Kubernetes","Minikube","Helm"]},{"location":"kubernetes/helm/getting-started-helm/#recherche-dune-chart","title":"Recherche d'une Chart","text":"<p>Maintenant que nous avons un d\u00e9p\u00f4t officiel, nous pouvons rechercher une application :</p> <pre><code>helm search repo wordpress\n\nNAME                    CHART VERSION   APP VERSION DESCRIPTION                                       \nbitnami/wordpress       22.4.18         6.5.5       WordPress is the world's most popular blogging ...\nbitnami/wordpress-intel 2.1.31          6.1.1       DEPRECATED WordPress for Intel is the most popu...\n</code></pre> <p>On remarque plusieurs champs :</p> <ul> <li><code>NAME</code> -&gt; correspond au nom du chart</li> <li><code>CHART VERSION</code> -&gt; la version du chart</li> <li><code>APP VERSION</code> -&gt; ici, la version officielle de wordpress</li> <li><code>DESCRIPTION</code> -&gt; une descritpion du logiciel</li> </ul>","tags":["Kubernetes","Minikube","Helm"]},{"location":"kubernetes/helm/getting-started-helm/#installer-une-chart-helm","title":"Installer une chart Helm","text":"<p>Nous allons installer b\u00eatement la chart Helm pour Wordpress :</p> <pre><code>helm install wordpress-example bitnami/wordpress --create-namespace --namespace wordpress-homelab\n</code></pre> <p>Si tout ce passe bien, la commande nous renvois diff\u00e9rentes informations, notament la m\u00e9thode pour se connecter \u00e0 wordpress avec les valeurs par d\u00e9faut.</p> <pre><code>NAME: wordpress-example\nLAST DEPLOYED: Mon Jul  8 16:55:56 2024\nNAMESPACE: wordpress-homelab\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCHART NAME: wordpress\nCHART VERSION: 22.4.18\nAPP VERSION: 6.5.5\n\n** Please be patient while the chart is being deployed **\n\nYour WordPress site can be accessed through the following DNS name from within your cluster:\n\n    wordpress-example.wordpress-homelab.svc.cluster.local (port 80)\n\nTo access your WordPress site from outside the cluster follow the steps below:\n\n1. Get the WordPress URL by running these commands:\n\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        Watch the status with: 'kubectl get svc --namespace wordpress-homelab -w wordpress-example'\n\n   export SERVICE_IP=$(kubectl get svc --namespace wordpress-homelab wordpress-example --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}\")\n   echo \"WordPress URL: http://$SERVICE_IP/\"\n   echo \"WordPress Admin URL: http://$SERVICE_IP/admin\"\n\n2. Open a browser and access WordPress using the obtained URL.\n\n3. Login with the following credentials below to see your blog:\n\n  echo Username: user\n  echo Password: $(kubectl get secret --namespace wordpress-homelab wordpress-example -o jsonpath=\"{.data.wordpress-password}\" | base64 -d)\n\nWARNING: There are \"resources\" sections in the chart not set. Using \"resourcesPreset\" is not recommended for production. For production installations, please set the following values according to your workload needs:\n  - resources\n+info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n</code></pre> <p>Par exemple, en utilisant minikube et le chart Wordpress, avec la commande suivante nous pouvons expos\u00e9 localement notre premier site :</p> <pre><code>k port-forward -n wordpress-homelab deployments/wordpress-example 8080:8080\n</code></pre>","tags":["Kubernetes","Minikube","Helm"]},{"location":"kubernetes/helm/template-helm/","title":"Les templates Helm","text":"<p>Un template Helm est une base  de configuration variabilis\u00e9 pour int\u00e9ragir dynamiquement avec des valeurs indiqu\u00e9es par d\u00e9fauts ou par l'utilisateur.</p> <p>Il faut voir ceci comme le fait de cr\u00e9er un mod\u00e8le dyniquement pour configurer une application en fonction du client et de son besoin.</p> <p>L'id\u00e9e est de faire comme sous Ansible avec les templates Jinja2, variabiliser et conditionner toutes nos configurations YAML.</p> <p>En passant par :</p> <ul> <li>D\u00e9ploimement (ou DaemonSet, ReplicatSetn etc..)</li> <li>Stockage (PV/PVC)</li> <li>Ingress (R\u00e8gle r\u00e9seau)</li> <li>Compte de Service (SA)</li> <li>Exposition du service (SVC)</li> </ul> <p>Bref, tous les \u00e9lements n\u00e9cessaires \u00e0 une application</p>","tags":["Kubernetes","Minikube","Helm"]},{"location":"kubernetes/minikube/","title":"Introduction","text":"<p>Minikube est un outil qui permet de lancer un cluster Kubernetes localement sur votre machine. C'est id\u00e9al pour les d\u00e9veloppeurs qui souhaitent tester ou d\u00e9velopper des applications Kubernetes sans avoir besoin de d\u00e9ploiement sur un vrai cluster. Minikube simule un environnement Kubernetes complet sur un PC ou Mac, facilitant l'apprentissage, le d\u00e9veloppement, et l'exp\u00e9rimentation avec Kubernetes.</p>"},{"location":"kubernetes/minikube/getting-started-minikube/","title":"Installation","text":"<p>Syst\u00e8me d'exploitation : Linux Ubuntu</p> <p>Rien de plus simple pour installation minikube sur son poste :</p> <pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube &amp;&amp; rm minikube-linux-amd64\n</code></pre>","tags":["Kubernetes","Minikube"]},{"location":"kubernetes/minikube/getting-started-minikube/#premier-cluster","title":"Premier cluster","text":"<p>Pour mettre en place son premier Cluster via minikube sur sa machine, en fonction des ressources, il suffit de lancer cette commande :</p> <pre><code>minikube start --cpus=2 --memory=2024\n</code></pre> <p>Attention, il faut au minimum 2 Coeurs ainsi que 1800Mo de m\u00e9moire.</p> <p>Une fois quelques minutes pass\u00e9es, notre Cluster est pr\u00eat :</p> <pre><code>$kubectl get nodes\nNAME           STATUS   ROLES                  AGE   VERSION\nminikube       Ready    control-plane          11d   v1.30.0\n</code></pre>","tags":["Kubernetes","Minikube"]},{"location":"kubernetes/minikube/getting-started-minikube/#en-avant","title":"En avant !","text":"<p>L'acc\u00e8s au cluster est automatiquement configur\u00e9 au travers du fichier kubeconfig se trouvant ici : $HOME/.kube/config</p> <p>Pour plus de rapidit\u00e9, nous pouvons cr\u00e9er un alias k=kubectl afin de r\u00e9duire la taille de nos commandes.</p> <p>On peut aussi activer l'autocompl\u00e9tion pour la commande kubectl :</p> <p><pre><code>source &lt;(kubectl completion bash)\n# Ainsi que l'autocompl\u00e9tion pour notre alias\ncomplete -o default -F __start_kubectl k\n</code></pre> Vous \u00eatre maintenant pr\u00eat \u00e0 manipuler Kube !</p>","tags":["Kubernetes","Minikube"]},{"location":"kubernetes/tools/","title":"Kubernetes, outils et configurations","text":"<p>Cette section va permettre de d\u00e9crire les diff\u00e9rents outils pratiques dans la gestion de Kubernetes.</p>"},{"location":"kubernetes/tools/k8s-k9s/","title":"k9s - Un outil pratique","text":"<p>Source : k9s</p> <p>k9s est une interface int\u00e9rative int\u00e9gr\u00e9 dans le shell pour observer et manipuler un cluster Kubernetes.</p> <p>L'objetif principale ici est de simplifier la navigation au travers des objets dans Kubernetes et aussi de simplifier la mani\u00e8re dont on observer les \u00e9venements.</p>","tags":["Kubernetes"]},{"location":"kubernetes/tools/k8s-k9s/#installation","title":"Installation","text":"<p>On t\u00e9l\u00e9charge le binaire depuis Github et on l'installe en fonction de notre plateforme</p> <p>Debian :</p> <pre><code>sudo dpkg -i k9s_linux_amd64.deb\n</code></pre>","tags":["Kubernetes"]},{"location":"kubernetes/tools/k8s-k9s/#manipulation","title":"Manipulation","text":"<p>Rien de plus simple, k9s se base sur le contexte courant, donc vous avez juste \u00e0 lancer la commande k9s.</p> <p>Cela nous donnes :</p> <p></p> <p>L'outils k9s est pratique dans le sens on l'on peut int\u00e9ragir avec le Cluster directement.  Si on appuie sur la touche \"s\", on va alors demander un shell int\u00e9ractif :</p> <p></p> <p>Ensuite, si l'on souhaite consulter le d\u00e9ploiement sur un pod particulier (dans mon cas, wordpress en Statefulset) :</p> <p></p>","tags":["Kubernetes"]},{"location":"kubernetes/tools/k8s-kubespy/","title":"kubespy | Suivez-vos d\u00e9ploiement","text":"<p>Source : kubespy</p> <p>KubeSpy permet de suivre l'activit\u00e9 sur un Cluster Kubernetes lors d'un d\u00e9ploiement ou d'une ressource.</p>","tags":["Kubernetes"]},{"location":"kubernetes/tools/k8s-kubespy/#installation","title":"Installation","text":"<p>T\u00e9l\u00e9charger le binaire en fonction de votre distribution puis :</p> <pre><code># Installation sous Debian 12\n\nsudo mv ./kubespy /usr/local/bin/\n\n# Activation de l'autocompl\u00e9tion \n\nkubespy completion bash &gt; kubespy.bash\nsudo mv kubespy.bash /usr/share/bash-completion/completions/\n\n# Ajouter cette ligne dans votre .bashrc :\n\nsource /usr/share/bash-completion/completions/\n</code></pre>","tags":["Kubernetes"]},{"location":"kubernetes/tools/k8s-kubespy/#manipulation","title":"Manipulation","text":"<p>Si on lance kubespy avec l'option trace pour suivre l'\u00e9tat d'un d\u00e9ploiement, cela nous donnes :</p> <pre><code>kubespy trace deploy wordpress-example\n[ADDED apps/v1/Deployment]  wordpress-homelab/wordpress-example\n    Rolling out Deployment revision 1\n    \u2705 Deployment is currently available\n    \u2705 Rollout successful: new ReplicaSet marked 'available'\n\nROLLOUT STATUS:\n- [Current rollout | Revision 1] [ADDED]  wordpress-homelab/wordpress-example-64d4bdfddc\n    \u2705 ReplicaSet is available [1 Pods available of a 1 minimum]\n       - [Ready] wordpress-example-64d4bdfddc-7vcwt\n</code></pre>","tags":["Kubernetes"]},{"location":"kubernetes/tools/k8s-tools/","title":"Gestion des contextes","text":"","tags":["Kubernetes"]},{"location":"kubernetes/tools/k8s-tools/#contexte-kube","title":"Contexte kube","text":"<p>Au travers du binaire kubectl, nous pouvons configurer plusieurs contextes, c'est \u00e0 dire plusieurs configuration qui d\u00e9crivent la connexion \u00e0 un cluster Kubernetes distant.</p> <p>Pour g\u00e9rer les contextes, il faut pr\u00e9fixer les commandes par <code>kubectl config</code>.</p> <p>Par exemple, si l'on souhaite afficher les contextes :</p> <pre><code>k config current-context\n</code></pre> <p>On peut afficher la liste des contextes d\u00e9j\u00e0 configur\u00e9s :</p> <pre><code>k config get-contexts\n\nCURRENT   NAME       CLUSTER    AUTHINFO   NAMESPACE\n*         minikube   minikube   minikube   default\n</code></pre> <p>Dans notre, nous sommes sur minikube donc c'est par d\u00e9faut notre contexte et notre Cluster.</p> <p>Par d\u00e9faut c'est le fichier plac\u00e9 sous <code>~/.kube/config</code>qui est pris en compte.</p> <p>Il existe une variable globale qui permet de g\u00e9rer les fichiers de configuration \u00e0 prendre en compte : KUBECONFIG</p> <pre><code>export KUBECONFIG=\"/kubernetes/mon-super-cluster\"\n\nk config get-contexts\n\nCURRENT   NAME                CLUSTER             AUTHINFO   NAMESPACE\n          mon-super-cluster   mon-super-cluster   minikube   default\n</code></pre> <p>Si l'on souhaite changer de contexte, il suffit de passer par la commande : <code>kubectl config use-context &lt;contexte&gt;</code>.</p> <p>Cr\u00e9ation d'un nouveau contexte :</p> <pre><code>k config set-context minikube \\\n&gt; --cluster=minikube \\\n&gt; --user=minikube \\\n&gt; --namespace=default\nContext \"minikube\" created.\n</code></pre> <p>Ensuite, on choisi le nouveau contexte :</p> <pre><code>k config use-context minikube\nSwitched to context \"minikube\"\n</code></pre>","tags":["Kubernetes"]},{"location":"kubernetes/tools/k8s-tools/#kubectx-kubens","title":"kubectx &amp; kubens","text":"<p>Source : Github</p> <p>kubens permet de se d\u00e9placer au travers des diff\u00e9rents namespaces plus facilement. C'est un script \u00e9crit en bash qui permet de simplier la gestion des namespaces.</p> <p>kubectx permet de se d\u00e9placer au travers de tous les contextes configur\u00e9s localement. </p> <p>Installation :</p> <pre><code>sudo apt install kubectx\n\nsudo wget -O /usr/local/bin/kubens https://github.com/ahmetb/kubectx/blob/master/kubens\nsudo chmod +x /usr/local/bin/kubens\n</code></pre> <p>Activation de l'autocompl\u00e9tion :</p> <pre><code>cd /usr/share/bash-completion/completions\nsudo vi kubectx.bash\n\n# Ins\u00e9rer le code suivant :\n_kube_contexts()\n{\n  local curr_arg;\n  curr_arg=${COMP_WORDS[COMP_CWORD]}\n  COMPREPLY=( $(compgen -W \"- $(kubectl config get-contexts --output='name')\" -- $curr_arg ) );\n}\n\ncomplete -F _kube_contexts kubectx kctx\n</code></pre> <p>Dans votre .bashrc, il faudra ajouter les lignes suivantes :</p> <pre><code>source /usr/share/bash-completion/completions/kubectx.bash\nsource /usr/share/bash-completion/completions/kubens.bash\n</code></pre>","tags":["Kubernetes"]},{"location":"linux/","title":"Linux","text":"<p>Ici on parle Linux et \u00e9cosyst\u00e8me !</p>"},{"location":"linux/cluster-rhel-httpd/","title":"Objectifs :","text":"<ul> <li>Cr\u00e9ation d\u2019un Cluster sous Linux EL8 ou RockyLinux 8 avec pacemaker sans Fencing.</li> <li>Cr\u00e9ation d\u2019une ressource LVM</li> <li>Cr\u00e9ation d\u2019une IP Virtuelle partag\u00e9e</li> <li>Cr\u00e9ation d\u2019un service web mutualis\u00e9</li> </ul> <p>Source : RedHat Cluster Active-Passive HTTPD EL8 with HA</p>","tags":["Linux","Apache","Enterprise Linux 8","Vagrant"]},{"location":"linux/cluster-rhel-httpd/#environement","title":"Environement :","text":"<p>Dans le cadre du homelab, nous allons cr\u00e9\u00e9er deux VMs via Vagrant avec le fichier suivant :</p> <pre><code>Vagrant.configure(\"2\") do |config|\n  config.vm.box = \"generic/rocky8\"\n\n  # Configuration du plugin disksize pour ajuster la taille du disque principal\n  config.disksize.size = '40GB'\n\n  config.vm.provider \"virtualbox\" do |hv|\n    hv.cpus = 1\n    hv.memory = 512  \n  end\n\n  config.vm.define \"ha01\" do |ha01|\n    ha01.vm.network :public_network, ip: \"192.168.1.151\", bridge: \"eno1\"\n    ha01.vm.hostname = \"clusha01.lab.example.priv\"\n    # Configuration pour ajouter un disque suppl\u00e9mentaire, si n\u00e9cessaire\n    ha01.vm.disk :disk, name: \"ha01-nfs\", size: \"10GB\"\n  end\n\n  config.vm.define \"ha02\" do |ha02|\n    ha02.vm.network :public_network, ip: \"192.168.1.152\", bridge: \"eno1\"\n    ha02.vm.hostname = \"clusha02.lab.example.priv\"\n    # Configuration pour ajouter un disque suppl\u00e9mentaire, si n\u00e9cessaire\n    ha02.vm.disk :disk, name: \"ha02-nfs\", size: \"10GB\"\n  end\nend\n</code></pre> <p>On aura donc deux machines avec une IP fixe et un disque de 10G</p> Machine IP CPU M\u00e9moire OS Disque LVM clusha01.lab.example.com 192.168.1.151 1 512 Mb RockyLinux 8.9 10G clusha02.lab.example.com 192.168.1.152 1 512 Mb RockyLinux 8.9 10G","tags":["Linux","Apache","Enterprise Linux 8","Vagrant"]},{"location":"linux/cluster-rhel-httpd/#prerequis-os","title":"Pr\u00e9requis OS","text":"<p>Activation du d\u00e9p\u00f4t High Availability :</p> <pre><code>sudo dnf config-manager --set-enabled ha\n</code></pre> <p>Installation des paquets n\u00e9cessaires :</p> <pre><code>dnf install -y pcs pacemaker\n</code></pre> <p>Activation des services au d\u00e9marrage :</p> <pre><code>sudo systemctl enable --now pcsd.service\n</code></pre> <p>Ouverture des ports n\u00e9cessaires :</p> <pre><code>sudo firewall-cmd --permanent --add-service=high-availability\nsudo firewall-cmd --reload\n</code></pre> <p>Configuration de la r\u00e9solution DNS en local (si besoin) :</p> <pre><code># Ajouter ces deux lignes si vous n'avez pas de DNS sur chacun des deux noeuds :\n192.168.1.151 clusha01.lab.example.priv clusha01\n192.168.1.152 clusha02.lab.example.priv clusha02\n</code></pre>","tags":["Linux","Apache","Enterprise Linux 8","Vagrant"]},{"location":"linux/cluster-rhel-httpd/#configuration-de-pacemaker","title":"Configuration de Pacemaker","text":"<p>Changement du mot de passe pour le compte hacluster :</p> <pre><code>passwd hacluster\n</code></pre> <p>Authentification du compte hacluster sur les deux machines :</p> <pre><code>pcs host auth clusha01.lab.example.com clusha02.lab.example.com\n# Retour de la commande :\nUsername: hacluster\nPassword: \nclusha02.lab.example.priv: Authorized\nclusha01.lab.example.priv: Authorized\n</code></pre> <p>Cr\u00e9ation du Cluster sur les deux machines :</p> <pre><code># Lancer la commande sur une seule des deux machines :\npcs cluster setup mon_cluster --start clusha01.lab.example.priv clusha02.lab.example.priv\n# Le syst\u00e8me nous retourne ceci :\no addresses specified for host 'clusha01.lab.example.priv', using 'clusha01.lab.example.priv'\nNo addresses specified for host 'clusha02.lab.example.priv', using 'clusha02.lab.example.priv'\nDestroying cluster on hosts: 'clusha01.lab.example.priv', 'clusha02.lab.example.priv'...\nclusha01.lab.example.priv: Successfully destroyed cluster\nclusha02.lab.example.priv: Successfully destroyed cluster\nRequesting remove 'pcsd settings' from 'clusha01.lab.example.priv', 'clusha02.lab.example.priv'\nclusha01.lab.example.priv: successful removal of the file 'pcsd settings'\nclusha02.lab.example.priv: successful removal of the file 'pcsd settings'\nSending 'corosync authkey', 'pacemaker authkey' to 'clusha01.lab.example.priv', 'clusha02.lab.example.priv'\nclusha01.lab.example.priv: successful distribution of the file 'corosync authkey'\nclusha01.lab.example.priv: successful distribution of the file 'pacemaker authkey'\nclusha02.lab.example.priv: successful distribution of the file 'corosync authkey'\nclusha02.lab.example.priv: successful distribution of the file 'pacemaker authkey'\nSending 'corosync.conf' to 'clusha01.lab.example.priv', 'clusha02.lab.example.priv'\nclusha01.lab.example.priv: successful distribution of the file 'corosync.conf'\nclusha02.lab.example.priv: successful distribution of the file 'corosync.conf'\nCluster has been successfully set up.\nStarting cluster on hosts: 'clusha01.lab.example.priv', 'clusha02.lab.example.priv'...\n</code></pre> <p>D\u00e9sactivation de stonith qui est utile quand nous avons une 3\u00e8me machine d\u00e9di\u00e9e au quorum :</p> <pre><code>pcs property set stonith-enabled=falseActivation des services du Cluster au d\u00e9marrage :\n</code></pre> <pre><code>pcs cluster enable --all\n</code></pre> <p>Cette commande n\u2019est pas obligatoire dans le sens o\u00f9 si l\u2019on souhaite un \u00e9tat diff\u00e9rent au red\u00e9marage d\u2019un des noeuds, par exemple que les services ne soient pas d\u00e9marr\u00e9s.</p> <p>Afficher l\u2019\u00e9tat actuel de notre Cluster :</p> <pre><code>pcs cluster status\n# R\u00e9sultat :\nCluster Status:\n Cluster Summary:\n   * Stack: corosync (Pacemaker is running)\n   * Current DC: clusha01.lab.example.priv (version 2.1.6-9.el8_9-6fdc9deea29) - partition with quorum\n   * Last updated: Sat Feb 10 10:17:58 2024 on clusha01.lab.example.priv\n   * Last change:  Sat Feb 10 10:17:56 2024 by root via cibadmin on clusha01.lab.example.priv\n   * 2 nodes configured\n   * 0 resource instances configured\n Node List:\n   * Online: [ clusha01.lab.example.priv clusha02.lab.example.priv ]\n\nPCSD Status:\n  clusha01.lab.example.priv: Online\n  clusha02.lab.example.priv: Online\n</code></pre> <p>Cr\u00e9ation d\u2019une premi\u00e8re sauvegarde pour le Cluster via la commande :</p> <pre><code>pcs config backup cluster_sauvegarde\n</code></pre> <p>Un fichier est alors cr\u00e9\u00e9 \u00e0 la racine de notre compte actuel (root) :</p> <pre><code>-rw-------. 1 root root 1839 Feb 10 10:21 cluster_sauvegarde.tar.bz2\n</code></pre>","tags":["Linux","Apache","Enterprise Linux 8","Vagrant"]},{"location":"linux/cluster-rhel-httpd/#configuration-dun-cluster-actifpassif-httpd","title":"Configuration d\u2019un Cluster Actif/Passif HTTPD","text":"","tags":["Linux","Apache","Enterprise Linux 8","Vagrant"]},{"location":"linux/cluster-rhel-httpd/#creation-dun-filesystem-partage-lvm-via-pacemaker","title":"Cr\u00e9ation d\u2019un filesystem partag\u00e9 LVM via pacemaker","text":"<p>D\u00e9claration du systemid dans la configuration de lvm pour identifier chaque machine :</p> <pre><code># vi /etc/lvm/lvm.conf\n# D\u00e9commenter et modifier la ligne :\nsystem_id_source = \"uname\"\n\n# R\u00e9sultat :\n[root@clusha01 ~]# lvm systemid\n  system ID: clusha01.lab.example.priv\n</code></pre> <p>Cr\u00e9ation du vg_shared et d\u00e9sactivation du montage automatique au d\u00e9marrage de la machine :</p> <p>(Chaque machine poss\u00e8de un disque de 10G sur /dev/sdb)</p> <pre><code>vgcreate --setautoactivation n vg_shared /dev/sdb\n\n# R\u00e9sultat :\nPhysical volume \"/dev/sdb\" successfully created.\n  Volume group \"vg_shared\" successfully created with system ID clusha01.lab.example.priv\n</code></pre> <p>Cr\u00e9ation du lv_shared avec la taille total de notre vg_shared :</p> <pre><code>lvcreate -n lv_shared -l 100%FREE vg_shared\n\n# R\u00e9sultat :\nLogical volume \"lv_shared\" created.\n</code></pre> <p>Cr\u00e9ation du syst\u00e8me de fichier de type xfs sur notre lv_shared :</p> <pre><code>mkfs.xfs /dev/vg_shared/lv_shared\n</code></pre>","tags":["Linux","Apache","Enterprise Linux 8","Vagrant"]},{"location":"linux/cluster-rhel-httpd/#configuration-de-apache-httpd","title":"Configuration de Apache HTTPD","text":"<p>Installation du paquet :</p> <pre><code>dnf install -y httpd wget\n</code></pre> <p>Ouvertures des portes du parefeu n\u00e9cessaires :</p> <pre><code>firewall-cmd --permanent --add-service=http\nfirewall-cmd --permanent --zone=public --add-service=http\nfirewall-cmd --reload\n</code></pre> <p>Ajout d\u2019une configuration pour /etc/httpd/conf.d/status.conf pour r\u00e9cup\u00e9rer l\u2019\u00e9tat du serveur Apache :</p> <pre><code># Passer la commande :\ncat &lt;&lt;-END &gt; /etc/httpd/conf.d/status.conf\n&lt;Location /server-status&gt;\n    SetHandler server-status\n    Require local\n&lt;/Location&gt;\nEND\n</code></pre> <p>Monter le volume lvm dans /var/www pour la redondance des fichiers Apache, sur une seule machine :</p> <pre><code>mount /dev/vg_shared/lv_shared /var/www\nmkdir -p /var/www/{html,cgi-bin,error}\nrestorecon -R /var/www\n</code></pre> <p>Ajouter une page web tr\u00e8s basic au serveur Apache :</p> <pre><code>#\u00a0Passer la commande suivante :\ncat &lt;&lt;-END &gt;/var/www/html/index.html\n&lt;html&gt;\n&lt;body&gt;Hello&lt;/body&gt;\n&lt;/html&gt;\nEND\n</code></pre> <p>Enfin, d\u00e9monter le volume :</p> <pre><code>umount /var/www\n</code></pre>","tags":["Linux","Apache","Enterprise Linux 8","Vagrant"]},{"location":"linux/cluster-rhel-httpd/#creation-des-resources-partages-via-pacemaker","title":"Cr\u00e9ation des resources partag\u00e9s via pacemaker","text":"<p>Cr\u00e9ation d\u2019une premi\u00e8re ressource de type syst\u00e8me de fichier partag\u00e9 pour notre partie Apache, avec le LV lv_shared que nous avons cr\u00e9\u00e9 pr\u00e9c\u00e9dement :</p> <pre><code># Sur une seule machine, taper les commandes :\npcs resource create lvm_shared ocf:heartbeat:LVM-activate vgname=vg_shared vg_access_mode=system_id --group apachegroup\n# Ce qui nous donne :\n[root@clusha01 ~]# pcs resource status\n  * Resource Group: apachegroup:\n    * lvm_shared    (ocf::heartbeat:LVM-activate):   Started clusha01.lab.example.priv\n</code></pre> <p>Cr\u00e9ation des 3 ressources principales,</p> <ul> <li>Ip Virtuelle</li> <li>Syst\u00e8me de Fichier</li> <li>Apache</li> </ul> <pre><code>pcs resource create fs_shared Filesystem device=\"/dev/vg_shared/lv_shared\" directory=\"/var/www\" fstype=\"xfs\" --group apachegroup\npcs resource create VirtualIP IPaddr2 ip=192.168.1.249 cidr_netmask=24 --group apachegroup\nsyst\n</code></pre> <p>Si nous n\u2019avons pas d\u2019erreur, alors le r\u00e9sutlat doit \u00eatre le suivant :</p> <pre><code>[root@clusha01 ~]# pcs status\nCluster name: mon_cluster\nCluster Summary:\n  * Stack: corosync (Pacemaker is running)\n  * Current DC: clusha01.lab.example.priv (version 2.1.6-9.el8_9-6fdc9deea29) - partition with quorum\n  * Last updated: Sun Feb 11 20:03:21 2024 on clusha01.lab.example.priv\n  * Last change:  Sun Feb 11 20:02:31 2024 by root via cibadmin on clusha01.lab.example.priv\n  * 2 nodes configured\n  * 4 resource instances configured\n\nNode List:\n  * Online: [ clusha01.lab.example.priv clusha02.lab.example.priv ]\n\nFull List of Resources:\n  * Resource Group: apachegroup:\n    * lvm_shared    (ocf::heartbeat:LVM-activate):   Started clusha01.lab.example.priv\n    * fs_shared (ocf::heartbeat:Filesystem):     Started clusha01.lab.example.priv\n    * VirtualIP (ocf::heartbeat:IPaddr2):    Started clusha01.lab.example.priv\n    * Website   (ocf::heartbeat:apache):     Started clusha01.lab.example.priv\n\nDaemon Status:\n  corosync: active/enabled\n  pacemaker: active/enabled\n  pcsd: active/enabled\n</code></pre> <p>Notre VIP (Virtual IP) est bien cr\u00e9\u00e9e :</p> <pre><code>[root@clusha01 ~]# ip a |grep eth1\n3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    inet 192.168.1.151/24 brd 192.168.1.255 scope global noprefixroute eth1\n    inet 192.168.1.249/24 brd 192.168.1.255 scope global secondary eth1\n</code></pre> <p>On peut essayer de joindre notre serveur Web avec la commande cURL :</p> <pre><code>curl -XGET http://192.168.1.249/index.html\n\n# Retour :\n&lt;html&gt;\n&lt;body&gt;Hello&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Fracture volontaire d\u2019un n\u0153ud pour prouver la r\u00e9silience :</p> <pre><code>pcs node standby clusha01.lab.example.priv\n# La VIP bascule normalement sur l'autre machine\nFull List of Resources:\n  * Resource Group: apachegroup:\n    * lvm_shared    (ocf::heartbeat:LVM-activate):   Started clusha02.lab.example.priv\n    * fs_shared (ocf::heartbeat:Filesystem):     Started clusha02.lab.example.priv\n    * VirtualIP (ocf::heartbeat:IPaddr2):    Started clusha02.lab.example.priv\n    * Website   (ocf::heartbeat:apache):     Started clusha02.lab.example.priv\n</code></pre> <p>Si on coupe le serveur distant :</p> <pre><code>Node List:\n   * Node clusha01.lab.example.priv: OFFLINE (standby)\n   * Online: [ clusha02.lab.example.priv ]\n\nPCSD Status:\n  clusha02.lab.example.priv: Online\n  clusha01.lab.example.priv: Offline\n</code></pre> <p>Le service est toujours rendu par la machine en vie.</p>","tags":["Linux","Apache","Enterprise Linux 8","Vagrant"]},{"location":"linux/freeipa/","title":"Introduction","text":"<p>FreeIPA vise \u00e0 fournir un syst\u00e8me d'identit\u00e9, de politique et d'audit (IPA) g\u00e9r\u00e9 de mani\u00e8re centralis\u00e9e3. Il utilise une combinaison de Fedora, 389 Directory Server, MIT Kerberos, NTP, DNS, l'IGC DogTag, SSSD et d'autres composants open-source libres. </p> <p>FreeIPA apporte des interfaces de gestion extensibles (CLI, interface utilisateur Web, XML-RPC et l'API JSONRPC), le SDK Python pour l'autorit\u00e9 de certification int\u00e9gr\u00e9e, et BIND avec un plug-in personnalis\u00e9 pour le serveur DNS int\u00e9gr\u00e9. </p> <p>Chacun des principaux composants de FreeIPA fonctionne comme un projet gratuit / open-source pr\u00e9existant. Le regroupement de ces composants dans une seule suite avec une interface de gestion compl\u00e8te est sous licence GPLv3, mais cela ne change pas les licences des composants.</p> <p>Depuis la version 3.0.0, FreeIPA utilise Samba pour s'int\u00e9grer \u00e0 Active Directory Microsoft par le biais de Cross Forest Trusts. FreeIPA prend en charge les ordinateurs Linux, Unix, Windows et Mac OS X. </p> <p>Sources: </p> <ul> <li>Wikipedia</li> <li>FreeIPA</li> </ul>"},{"location":"linux/freeipa/#composants","title":"Composants","text":"Composant D\u00e9tails Enterprise Linux Syst\u00e8me d'exploitation GNU / Linux 389 Directory Server Impl\u00e9mentation LDAP Kerberos 5 (MIT) Authentification et SSO Serveur HTTP Apache Interface utilisateur Web et Infrastructure logicielle Python Infrastructure logicielle DogTab Autorit\u00e9 de certification interne (PKI) Samba/Winbind Int\u00e9gration Active Directory Named Int\u00e9gration DNS PKCS#11 API Cryptographique pour FreeIPA <p>Cette ensemble de services permet d'apporter une solution libre et opensource aux produits nativements int\u00e9gr\u00e9s c\u00f4t\u00e9 Windows avec l'AD/DNS et les composants gravitants.</p> <p>Le projet est d\u00e9velopp\u00e9 par RedHat, le produit officiel est RedHat Identity Management.</p>"},{"location":"linux/freeipa/freeipa-installation/","title":"Objectif","text":"<p>Le principe ici sera de montrer comment installer un serveur et un r\u00e9plica de FreeIPA.</p>","tags":["Linux","FreeIPA","Enterprise Linux 8"]},{"location":"linux/freeipa/freeipa-installation/#topologie-freeipa","title":"Topologie FreeIPA","text":"<p>Pourquoi deux serveurs ? L'id\u00e9e est d'offrir une r\u00e9silence et une tol\u00e9rance de panne en cas de crash d'un service FreeIPA (DNS, LDAP, PKI ou autre) ou bien d'un serveur.</p> <p>Le principe est simple, le serveur primaire est install\u00e9 en premier, il est maitre de tous les services, une fois termin\u00e9, le secondaire est install\u00e9 \u00e0 son tour et rejoint la topologie avec synchronisation en temps r\u00e9el des objets LDAP.</p>","tags":["Linux","FreeIPA","Enterprise Linux 8"]},{"location":"linux/freeipa/freeipa-installation/#pre-requis","title":"Pr\u00e9-requis","text":"<p>Il faut se munir de deux serveurs avec un minimum de 4G de m\u00e9moire et 2 CPUs. Si vous souhaitez utiliser le mode de cryptographie le plus \u00e9l\u00e9v\u00e9 afin de s\u00e9curis\u00e9 les serveurs, il faut le faire en avance de phase.</p> <p>Mes serveurs sont configur\u00e9s pour fonctionner avec FIPS. Cela ne peut \u00eatre chang\u00e9 \u00e0 chaud une fois les serveurs install\u00e9s.</p> <p>Si vous utilisez des DNS externes pour r\u00e9soudre les noms de vos VM, alors pas de soucis, sinon ajouter les IP + FQDN dans le fichier /etc/hosts de chaque VM.</p>","tags":["Linux","FreeIPA","Enterprise Linux 8"]},{"location":"linux/freeipa/freeipa-installation/#installation-via-ansible","title":"Installation via Ansible","text":"<p>Depuis plusieurs version d\u00e9j\u00e0, nous avons la possibilit\u00e9 de proc\u00e9der \u00e0 l'installation au travers de playbooks Ansible \u00e9crit par RedHat et la communaut\u00e9 afin de g\u00e9rer plus facilement les diff\u00e9rents composants.</p> <p>Il suffira d'avoir un serveur Ansible ou bien le binaire Ansible sur votre poste local et de cloner le d\u00e9p\u00f4t Git qui se trouve ici :</p> <ul> <li>Ansible FreeIPA</li> </ul>","tags":["Linux","FreeIPA","Enterprise Linux 8"]},{"location":"linux/freeipa/freeipa-installation/#inventaire","title":"Inventaire","text":"<p>Une fois le d\u00e9p\u00f4t clon\u00e9, l'inventaire se trouve dans inventaire/. Dans notre contexte, nous allons utiliser le fichier host.cluster.</p> <p>Voici un exemple de configuration pour une topologie \u00e0 deux noeuds :</p> <pre><code>[ipaserver]\nlnx0001.lab.it-linux.priv\n\n[ipaserver:vars]\nipaserver_setup_ca=yes\nipaserver_setup_adtrust=yes\nipaserver_setup_kra=yes\nipaserver_setup_dns=yes\nipaserver_no_hbac_allow=yes\nipaserver_no_pkinit=no\nipaserver_no_ui_redirect=no\nipaserver_mem_check=no\nipaserver_random_serial_numbers=false\nipaserver_no_dnssec_validation=yes\nipaserver_enable_compat=yes\nipaserver_no_forwarders=yes\n\n[ipareplicas]\nlnx0002.lab.it-linux.priv\n\n[ipareplicas:vars]\nipaclient_servers=lnx0001.lab.it-linux.priv\nipaclient_force_join=yes\nipareplica_setup_adtrust=yes\nipareplica_setup_ca=yes\nipareplica_setup_kra=yes\nipareplica_setup_dns=yes\nipareplica_no_dnssec_validation=yes\nipareplica_enable_compat=yes\nipareplica_no_forwarders=yes\n\n[ipaclients]\n\n[ipaclients:vars]\nipaclient_allow_repair=yes\n\n\n[ipa:children]\nipaserver\nipareplicas\nipaclients\n\n[ipa:vars]\nipaadmin_password=un_autre_mot_de_passe_complexe\nipadm_password=un_super_mot_de_passe_complexe\nipaserver_domain=lab.it-linux.priv\nipaserver_realm=LAB.IT-LINUX.PRIV\n</code></pre> <p>Explication des variables pour le serveur primaire ipaservers:</p> Variable Description Valeur ipaserver_setup_ca Utilisation de la PKI Interne yes ipaserver_setup_adtrust Module d'int\u00e9gration \u00e0 l'AD yes ipaserver_setup_kra Module Vault (Secret) yes ipaserver_setup_dns Utilisation de la brique DNS avec Named yes ipaserver_no_hbac_allow Ne pas utiliser la r\u00e8gle ALLOW_ALL par d\u00e9faut yes ipaserver_no_pkinit Utilisation de la PKI Interne yes ipaserver_no_ui_redirect Direction Apache sur le nom FQDN no ipaserver_mem_check V\u00e9rification du minimum requis en m\u00e9moire no ipaserver_no_dnssec_validation Activer/D\u00e9sactiver la v\u00e9rification DNSSEC yes ipaserver_enable_compat X yes ipaserver_no_forwarders Configurer des DNS Globaux yes <p>On retrouvera presque les m\u00eames variables pour le ipareplica.</p> <p>Dans un sc\u00e9nario de tol\u00e9rance de panne et de haute disponibilit\u00e9, l'id\u00e9al est de configurer les m\u00eames services entre le primaire et le secondaire. </p> <p>Dans certain contexte, un r\u00e9plica peut \u00eatre configur\u00e9 uniquement pour certain service. Par exemple si on souhaite uniquement avec un r\u00e9plica avec les services de base et le DNS, c'est possible.</p> <p>L'option hidden permet aussi de configurer un serveur en mode cach\u00e9, techniquement injoignable par les clients Linux de fa\u00e7on traditionnel avec SSSD.</p> <p>Il restera les variables en bas de fichier pour d\u00e9finir les mots de passe et le nom du domaine LDAP et Kerberos.</p>","tags":["Linux","FreeIPA","Enterprise Linux 8"]},{"location":"linux/freeipa/freeipa-installation/#lancement-de-linstallation","title":"Lancement de l'installation","text":"<p>Une fois l'inventaire compl\u00e9t\u00e9, les cl\u00e9s SSH configur\u00e9es et les serveurs pr\u00eats, il suffit de passer cette commande :</p> <pre><code>ansible-playbook -i inventory/hosts.cluster install-cluster.yml\n</code></pre> <p>L'installation dure environ 20Minutes pour les deux serveurs. A ce stade, vous ne devriez pas rencontrer de probl\u00e8me particulier.</p>","tags":["Linux","FreeIPA","Enterprise Linux 8"]},{"location":"linux/freeipa/freeipa-installation/#verification-de-linstallation","title":"V\u00e9rification de l'installation","text":"<p>En ligne de commande :</p> <pre><code># ipactl status\n\nDirectory Service: RUNNING\nkrb5kdc Service: RUNNING\nkadmin Service: RUNNING\nnamed Service: RUNNING\nhttpd Service: RUNNING\nipa-custodia Service: RUNNING\npki-tomcatd Service: RUNNING\nsmb Service: RUNNING\nwinbind Service: RUNNING\nipa-otpd Service: RUNNING\nipa-ods-exporter Service: STOPPED\nods-enforcerd Service: RUNNING\nipa-dnskeysyncd Service: RUNNING\nipa: INFO: The ipactl command was successful\n</code></pre> <p>Connexion via Kerberos :</p> <pre><code># kinit admin\nPassword for admin@LAB.IT-LINUX.PRIV: \n\n# klist\nTicket cache: KCM:0\nDefault principal: admin@LAB.IT-LINUX.PRIV\n\nValid starting       Expires              Service principal\n08/15/2024 19:01:54  08/16/2024 18:20:28  krbtgt/LAB.IT-LINUX.PRIV@LAB.IT-LINUX.PRIV\n</code></pre> <p>On peut afficher la configuration de FreeIPA avec les commandes :</p> <pre><code># ipa config-show\n\n  Maximum username length: 32\n  Maximum hostname length: 64\n  Home directory base: /home\n  Default shell: /bin/sh\n  Default users group: ipausers\n  Default e-mail domain: lab.it-linux.priv\n  Search time limit: 2\n  Search size limit: 100\n  User search fields: uid,givenname,sn,telephonenumber,ou,title\n  Group search fields: cn,description\n  Enable migration mode: False\n  Certificate Subject base: O=LAB.IT-LINUX.PRIV\n  Password Expiration Notification (days): 4\n  Password plugin features: AllowNThash, KDC:Disable Last Success\n  SELinux user map order: guest_u:s0$xguest_u:s0$user_u:s0$staff_u:s0-s0:c0.c1023$sysadm_u:s0-s0:c0.c1023$unconfined_u:s0-s0:c0.c1023\n  Default SELinux user: unconfined_u:s0-s0:c0.c1023\n  Default PAC types: MS-PAC, nfs:NONE\n  IPA masters: lnx0001.lab.it-linux.priv, lnx0002.lab.it-linux.priv\n  IPA master capable of PKINIT: lnx0001.lab.it-linux.priv, lnx0002.lab.it-linux.priv\n  IPA CA servers: lnx0001.lab.it-linux.priv, lnx0002.lab.it-linux.priv\n  IPA CA renewal master: lnx0001.lab.it-linux.priv\n  IPA KRA servers: lnx0001.lab.it-linux.priv, lnx0002.lab.it-linux.priv\n  IPA DNS servers: lnx0001.lab.it-linux.priv, lnx0002.lab.it-linux.priv\n  IPA DNSSec key master: lnx0002.lab.it-linux.priv\n</code></pre>","tags":["Linux","FreeIPA","Enterprise Linux 8"]},{"location":"nas/","title":"Solution NAS OpenSource","text":""},{"location":"nas/#truenas-scale","title":"TrueNAS Scale","text":"<p>TrueNAS est un syst\u00e8me d'exploitation sous licence libre, bas\u00e9 sur FreeBSD (TrueNAS CORE) et la distribution Linux Debian (TrueNAS SCALE), destin\u00e9 aux serveurs de stockage en r\u00e9seau NAS1,2,3,4,5,6. Il supporte de nombreux protocoles : CIFS (Samba), FTP, NFS, rsync, AFP, iSCSI, rapport S.M.A.R.T. l'authentification d'utilisateurs locaux, et RAID Logiciel (dans de nombreuses variantes), dont l'utilisation de la gestion par ZFS de la redondance du syst\u00e8me de fichier. </p> <p>Source : https://fr.wikipedia.org/wiki/TrueNas</p>"},{"location":"nas/#alternative","title":"Alternative","text":"<p>Il existe d'autre solution libre, opensource ou propri\u00e9taire, voici une liste non exhaustive :</p> <ul> <li>TrueNAS Core</li> <li>OpenMediaVault</li> <li>Synology</li> <li>NetAPP</li> <li>RockStor</li> <li>OpenFiler</li> </ul> <p>Il en existe bien plus !</p>"},{"location":"nas/truenas-scale/aws-backup/","title":"Sauvegarde via AWS !","text":"<p>Comment effectuer une sauvegarde sur un Stockage S3 sous AWS ?</p> <p>Nous allons voir tout cela plus bas !</p> <p>Requis</p> <p>La cr\u00e9ation d'un compte AWS et de sa configuration ainsi que celle du stockage S3 ne sera pas trait\u00e9 dans cette partie.</p>","tags":["AWS","TrueNAS"]},{"location":"nas/truenas-scale/aws-backup/#identifiant-cloud-aws","title":"Identifiant Cloud AWS","text":"<p>Dans un premier temps, il faut cr\u00e9er un identifiant Cloud AWS dans TrueNAS.</p> <p>Il faut se rendre dans l'onglet \"Identifians/Informations d'identification de sauvegarde\", puis dans Identifiants Cloud :</p> <p></p> <p>Il faudra entrer les informations que vous aurez obtenu depuis AWS \u00e0 la suite de la cr\u00e9ation d'un bucket S3 ainsi que d'un compte pour y acc\u00e9der depuis l'ext\u00e9rieur.</p>","tags":["AWS","TrueNAS"]},{"location":"nas/truenas-scale/aws-backup/#configuration-dune-sauvegarde","title":"Configuration d'une sauvegarde","text":"<p>Rien de plus simple pour notre premi\u00e8re sauvegarde dans le Cloud !</p> <p>On se rend dans Protection des donn\u00e9es puis on clique sur Ajouter sur le bloc T\u00e2ches Sync Cloud :</p> <p>Fournisseur : Amazon S3</p> <p>Ensuite on peut configurer la sauvegarde de plusieurs mani\u00e8res diff\u00e9rentes, soit en mode PUSH (Envoi) ou alors en mode PULL (R\u00e9cup\u00e9ration). Nous avons le choix sur le mode de transfert, on garde la source apr\u00e8s copie vers la destination, ou alors on suprime sur la source et on garde uniquement la destination, etc...</p> <p>Nous souhaitons ici copier des donn\u00e9es sur notre NAS vers une source externe pour la protection en cas de perte.</p> <p>Il suffit donc de configurer comme ci-dessous :</p> <p></p> <p>Vous pouvez choisir manuellement une planification. En fonction des donn\u00e9es, il faut \u00eatre vigilant, par exemple il ne faut pas envoyer un dossier contenant des films et qui pourrait grossir au fil du temps suivant vos ajouts personnels.</p> <p>Dans mon cas, je r\u00e9gule mon dossier \u00e0 \u00eatre envoy\u00e9 uniquement une fois par semaine.</p> <p>Budget AWS S3</p> <p>Le Stockage AWS S3 est payant \u00e0 l'usage, c'est \u00e0 dire que plus vous aurez d'espace de stockage utilis\u00e9 ou m\u00eame un grand nombre de requ\u00eates, cela vous sera d\u00e9duit chaque mois! Il est important de rester en maitrise de son stockage.</p>","tags":["AWS","TrueNAS"]},{"location":"nas/truenas-scale/k3s-cli/","title":"Introduction","text":"<p>Ici, nous allons voir comment int\u00e9ragir avec le cluster Kubernetes K3S auto-g\u00e9r\u00e9 dans la solution NAS TrueNAS et sa variante Scale.</p> <p>Notez que TrueNAS Core utilise encore des jails sous FreeBSD, c'est une sorte de \"virtualisation/conteneur\" \u00e0 la mode chroot. Cette page ne sera donc utile si vous \u00eates sous Core.</p>","tags":["Kubernetes","TrueNAS"]},{"location":"nas/truenas-scale/k3s-cli/#acces-a-la-ligne-de-commande","title":"Acc\u00e8s \u00e0 la ligne de commande","text":"<p>Pour ce faire, il y a deux moyens, le premier en passant par l'onglet Console sur l'interface Web ou le second en se connectant via ssh sur notre NAS.</p> <p>Nous allons passer par la console dans cette partie :</p> <p></p> <p>Il faut se rendre dans Param\u00e8tre syst\u00e8me puis Console.</p> <p>Ensuite, rien de plus simple, on utilise le binaire k3s suivi de la commabde kubectl standard pour int\u00e9roger Kubernetes:</p> <pre><code>sudo k3s kubectl get pods -n kube-system\n</code></pre> <p>Tada !</p> <p></p>","tags":["Kubernetes","TrueNAS"]},{"location":"nas/truenas-scale/k3s-storage/","title":"Introduction","text":"<p>Pour comprendre comment sont provisionn\u00e9s les volumes dans le Cluster Kubernetes sous TrueNAS, il faut comprendre quel est le backend officiel de stockage dans cette solution NAS.</p>","tags":["Kubernetes","TrueNAS","ZFS"]},{"location":"nas/truenas-scale/k3s-storage/#truenas-openzfs","title":"TrueNAS &amp; OpenZFS","text":"<p>Qu'est-ce que ZFS ?</p> <p>ZFS est un syst\u00e8me de fichiers. C'est aussi un gestionnaire de volumes. Cela signifie qu'il g\u00e8re le stockage depuis le disque jusqu'au syst\u00e8me d'exploitation, rempla\u00e7ant \u00e0 la fois les syst\u00e8mes de fichiers traditionnels comme FAT, NTFS, UFS ou ext4 et les solutions RAID traditionnelles comme les cartes RAID mat\u00e9rielles, le Drive Extender de Windows Home Server, les Storage Spaces de Windows 10, le GEOM de FreeBSD ou le Logical Volume Manager de Linux.</p> <p>Vous avez peut-\u00eatre entendu parler d'Oracle ZFS. ZFS a \u00e9t\u00e9 d\u00e9velopp\u00e9 chez Sun Microsystems par Matt Ahrens et Jeff Bonwick. Il a \u00e9t\u00e9 open-source et port\u00e9 sur d'autres syst\u00e8mes d'exploitation. Apr\u00e8s avoir acquis Sun, Oracle a pris la d\u00e9cision inhabituelle de fermer le d\u00e9veloppement interne du syst\u00e8me d'exploitation OpenSolaris, y compris ZFS. Le code existant \u00e9tait toujours open-source, donc la plupart de l'\u00e9quipe ZFS a quitt\u00e9 Oracle et le projet OpenZFS a \u00e9t\u00e9 cr\u00e9\u00e9 pour continuer le d\u00e9veloppement open-source de ZFS.</p>","tags":["Kubernetes","TrueNAS","ZFS"]},{"location":"nas/truenas-scale/k3s-storage/#classe-de-stockage","title":"Classe de stockage","text":"<p>Dans Kubernetes, on utilise des StorageClass ou Classe de Stockage en bon fran\u00e7ais. Cet objet permet de d\u00e9finir les informations n\u00e9cessaire pour r\u00e9clamer un espace de stockage. Il existe d'autre classe de stockage en fonction des fournisseurs, notamelent Trident pour NetAPP ou encore Portworx par PureStorage et d'autre encore.</p> <p>On peut v\u00e9rifier la classe de stockage par d\u00e9faut via la commande :</p> <p><pre><code>sudo k3s kubectl get storageclass\n</code></pre> On doit obtenir ceci :</p> <pre><code>NAME                              PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nopenebs-zfspv-default (default)   zfs.csi.openebs.io   Delete          Immediate           true                   90d\n</code></pre> <p>On peut afficher au format yaml la classe de stockage pour avoir plus d'infos :</p> <pre><code>apiVersion: v1\nitems:\n- allowVolumeExpansion: true\n  apiVersion: storage.k8s.io/v1\n  kind: StorageClass\n  metadata:\n    annotations:\n      storageclass.kubernetes.io/is-default-class: \"true\"\n    creationTimestamp: \"2024-03-31T17:50:06Z\"\n    name: openebs-zfspv-default\n    resourceVersion: \"18378374\"\n    uid: 95b00a4e-2e08-45e4-87b2-33d08e41e724\n  parameters:\n    fstype: zfs\n    poolname: NAS79_VOL_MIR_02/ix-applications/default_volumes\n    shared: \"yes\"\n  provisioner: zfs.csi.openebs.io\n  reclaimPolicy: Delete\n  volumeBindingMode: Immediate\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre> <p>Cette classe de stockage d\u00e9finit le pool ZFS configur\u00e9 dans TrueNAS Scale pour les applications Kubernetes \u00e0 utiliser, l'API va ensuite cr\u00e9er des volumes dynamiques en fonction des applications et des besoins.</p> <ul> <li>storageclass.kubernetes.io/is-default-class: \"true\"</li> </ul> <p>Permet de d\u00e9finir la classe par d\u00e9faut</p> <ul> <li>parameters.poolname: NAS79_VOL_MIR_02/ix-applications/default_volumes</li> </ul> <p>Permet de d\u00e9finir le pool ZFS pour tous les volumes Kube</p>","tags":["Kubernetes","TrueNAS","ZFS"]},{"location":"nas/truenas-scale/k3s-storage/#les-secrets","title":"Les secrets","text":"<p>Pour communiquer avec notre TrueNAS et ZFS, il faut que Kubernetes est acc\u00e8s \u00e0 l'API avec un compte et un mot de passe. </p> <p>Il existe un secret dans le namespace kube-system :</p> <pre><code>admin@scale:~$ sudo k3s kubectl get secret -n kube-system |grep -i truenas\n</code></pre> <pre><code>ix-truenas.node-password.k3s   Opaque              1      228d\n</code></pre> <p>On peut obtenir le mot de passe en regardant de plus pr\u00e8s le secret :</p> <pre><code>apiVersion: v1\ndata:\n  hash: \"une-tr\u00e8s-grande-chaine-de-caract\u00e8res-en-base64\"\nimmutable: true\nkind: Secret\nmetadata:\n  creationTimestamp: \"2023-11-14T18:38:52Z\"\n  name: ix-truenas.node-password.k3s\n  namespace: kube-system\n  resourceVersion: \"237\"\n  uid: ba5fb692-ffe8-4ff9-a831-23ee73616389\ntype: Opaque\n</code></pre> <p>Une envie de connaitre le mot de passe ?</p> <p>Rien de plus simple : </p> <pre><code>echo \"une-tr\u00e8s-grande-chaine-de-caract\u00e8res-en-base64\" | base64 -d\n</code></pre> <p>Et voil\u00e0 !</p>","tags":["Kubernetes","TrueNAS","ZFS"]},{"location":"nas/truenas-scale/k3s-storage/#les-customs-resources-definitions","title":"Les Customs Resources Definitions","text":"<p>On peut afficher la liste de CRD pour OpenEBS :</p> <pre><code>admin@scale:~$ sudo k3s kubectl get crd -n openebs\nNAME                                             CREATED AT\nhelmcharts.helm.cattle.io                        2023-11-14T18:38:50Z\nhelmchartconfigs.helm.cattle.io                  2023-11-14T18:38:50Z\naddons.k3s.cattle.io                             2023-11-14T18:38:50Z\nnetwork-attachment-definitions.k8s.cni.cncf.io   2023-11-14T18:38:54Z\nzfsbackups.zfs.openebs.io                        2023-11-14T18:38:56Z\nzfsnodes.zfs.openebs.io                          2023-11-14T18:38:56Z\nzfsrestores.zfs.openebs.io                       2023-11-14T18:38:56Z\nzfssnapshots.zfs.openebs.io                      2023-11-14T18:38:56Z\nzfsvolumes.zfs.openebs.io                        2023-11-14T18:38:56Z\nvolumesnapshotclasses.snapshot.storage.k8s.io    2023-11-14T18:38:54Z\nvolumesnapshotcontents.snapshot.storage.k8s.io   2023-11-14T18:38:54Z\nvolumesnapshots.snapshot.storage.k8s.io          2023-11-14T18:38:54Z\n</code></pre> <p>Si on regarde la ressource zfsnodes, on pourra voir notre NAS connect\u00e9 et ses pools :</p> <pre><code># sudo k3s kubectl describe zfsnodes -A\nName:         ix-truenas\nNamespace:    openebs\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  zfs.openebs.io/v1\nKind:         ZFSNode\nMetadata:\n  Creation Timestamp:  2023-11-14T18:42:13Z\n  Generation:          309892\n  Managed Fields:\n    API Version:  zfs.openebs.io/v1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:ownerReferences:\n          .:\n          k:{\"uid\":\"6a44f76f-c285-4e72-a604-47081266cd63\"}:\n      f:pools:\n    Manager:    zfs-driver\n    Operation:  Update\n    Time:       2024-06-30T16:56:46Z\n  Owner References:\n    API Version:     v1\n    Controller:      true\n    Kind:            Node\n    Name:            ix-truenas\n    UID:             6a44f76f-c285-4e72-a604-47081266cd63\n  Resource Version:  30619121\n  UID:               22466e9a-cc0c-42c3-a21e-f6088e1737c1\nPools:\n  Free:  2292652156Ki\n  Name:  NAS79_VOL_MIR_01\n  Uuid:  17172874061595888983\n  Free:  2709244160Ki\n  Name:  NAS79_VOL_MIR_02\n  Uuid:  10077988922093906049\n  Free:  9382090924512\n  Name:  NAS79_VOL_VAULT_01\n  Uuid:  16543390058926155067\n  Free:  445038332Ki\n  Name:  boot-pool\n  Uuid:  6920887177742642366\nEvents:  &lt;none&gt;\n</code></pre>","tags":["Kubernetes","TrueNAS","ZFS"]},{"location":"nas/truenas-scale/k3s-storage/#utiliser-les-volumes","title":"Utiliser les volumes","text":"<p>Comment le volume cr\u00e9\u00e9 dans un pool ZFS est ensuite mont\u00e9 dans un conteneur ? </p> <p>Le montage d'un volume type ix-application se fait via le mode hostPath, PVC ou NFS en fonction de l'application et de sa Chart.</p> <p>Quand le volume est cr\u00e9\u00e9 dynamiquement par la classe de stockage lors de l'installation d'une nouvelle application, le backend zfs cr\u00e9\u00e9 un nouveau volume et le monte sur le NAS lui m\u00eame. On peut afficher les volumes d\u00e9j\u00e0 mont\u00e9s :</p>","tags":["Kubernetes","TrueNAS","ZFS"]},{"location":"nas/truenas-scale/k3s-storage/#exemple-avec-hostpath","title":"Exemple avec hostPath","text":"<p>Type : hostPath</p> <pre><code># admin@scale:~$ df -h |grep -i \"ix-applications\" |awk '{print $1}'\n\nNAS79_VOL_MIR_02/ix-applications\nNAS79_VOL_MIR_02/ix-applications/default_volumes\nNAS79_VOL_MIR_02/ix-applications/k3s\nNAS79_VOL_MIR_02/ix-applications/catalogs\nNAS79_VOL_MIR_02/ix-applications/releases\nNAS79_VOL_MIR_02/ix-applications/k3s/kubelet\nNAS79_VOL_MIR_02/ix-applications/releases/prometheus\nNAS79_VOL_MIR_02/ix-applications/releases/prometheus/charts\nNAS79_VOL_MIR_02/ix-applications/releases/prometheus/volumes\nNAS79_VOL_MIR_02/ix-applications/releases/prometheus/volumes/ix_volumes\nNAS79_VOL_MIR_02/ix-applications/releases/prometheus/volumes/ix_volumes/data\nNAS79_VOL_MIR_02/ix-applications/releases/prometheus/volumes/ix_volumes/config\nNAS79_VOL_MIR_02/ix-applications/releases/grafana\nNAS79_VOL_MIR_02/ix-applications/releases/grafana/charts\nNAS79_VOL_MIR_02/ix-applications/releases/grafana/volumes\nNAS79_VOL_MIR_02/ix-applications/releases/grafana/volumes/ix_volumes\nNAS79_VOL_MIR_02/ix-applications/releases/grafana/volumes/ix_volumes/data\n</code></pre> <p>Ensuite une d\u00e9clartion dans le type de d\u00e9ploiement Kubernetes est ajout\u00e9 pour monter le volume, exemple avec une application Grafana basique :</p> <pre><code>...\n        volumeMounts:\n        - mountPath: /mnt/directories/data\n          name: data\n...\n      volumes:\n      - hostPath:\n          path: /mnt/NAS79_VOL_MIR_02/ix-applications/releases/grafana/volumes/ix_volumes/data\n          type: \"\"\n        name: data\n      - emptyDir: {}\n        name: tmp\n...\n</code></pre> <p>Comme on peut le voir dans cet exemple, on ne passe par la d\u00e9claration d'un PersistentVolumeClaim mais par hostPath. En fonction du type d'application et du type de stockage n\u00e9cessaire, la m\u00e9thode de provisionnement de stockage diff\u00e8re.</p>","tags":["Kubernetes","TrueNAS","ZFS"]},{"location":"nas/truenas-scale/k3s-storage/#exemple-avec-un-pvc","title":"Exemple avec un PVC","text":"<p>Ici, j'ai d\u00e9ploy\u00e9 une application via Chart pour qbittorrent, celle-ci n\u00e9cessite du stockage pour garder sa configuration persistante au red\u00e9marrage.</p> <p>Voici la composition du d\u00e9ploiement :</p> <pre><code>...\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: qbittorrent-config\n...\n</code></pre> <p>Ensuite, on peut afficher le PVC associ\u00e9 :</p> <pre><code># sudo k3s kubectl get pvc qbittorrent-config -n ix-qbittorrent\n\nNAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS            AGE\nqbittorrent-config   Bound    pvc-c58380a0-25d9-4080-a18f-e13b150edd13   5Gi        RWO            openebs-zfspv-default   130m\n</code></pre> <p>Voici sa composition :</p> <p><pre><code>#sudo k3s kubectl get pv -n ix-qbittorrent pvc-c58380a0-25d9-4080-a18f-e13b150edd13 --output yaml\n...\nspec:\n  accessModes:\n  - ReadWriteOnce\n  capacity:\n    storage: 5Gi\n  claimRef:\n    apiVersion: v1\n    kind: PersistentVolumeClaim\n    name: qbittorrent-config\n    namespace: ix-qbittorrent\n  csi:\n    driver: zfs.csi.openebs.io\n    fsType: zfs\n    volumeAttributes:\n      openebs.io/cas-type: localpv-zfs\n      openebs.io/poolname: NAS79_VOL_MIR_02/ix-applications/default_volumes\n      storage.kubernetes.io/csiProvisionerIdentity: 1719863288029-693-zfs.csi.openebs.io\n    volumeHandle: pvc-c58380a0-25d9-4080-a18f-e13b150edd13\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: openebs.io/nodeid\n          operator: In\n          values:\n          - ix-truenas\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: openebs-zfspv-default\n  volumeMode: Filesystem\n...\n</code></pre> On peut aussi utiliser une ressource (CRD) de OpenEBS pour afficher le volume : </p> <pre><code># sudo k3s kubectl get zv -n openebs\n\nNAME                                       ZPOOL                                              NODEID       SIZE         STATUS   FILESYSTEM   AGE\npvc-c58380a0-25d9-4080-a18f-e13b150edd13   NAS79_VOL_MIR_02/ix-applications/default_volumes   ix-truenas   5368709120   Ready    zfs          133m\n</code></pre> <p><pre><code># sudo k3s kubectl get zv -n openebs pvc-c58380a0-25d9-4080-a18f-e13b150edd13 --output yaml\n\npiVersion: zfs.openebs.io/v1\nkind: ZFSVolume\nmetadata:\n  creationTimestamp: \"2024-07-02T16:45:58Z\"\n  finalizers:\n  - zfs.openebs.io/finalizer\n  generation: 2\n  labels:\n    kubernetes.io/nodename: ix-truenas\n  name: pvc-c58380a0-25d9-4080-a18f-e13b150edd13\n  namespace: openebs\n  resourceVersion: \"30837917\"\n  uid: 2716d6ac-7607-4dad-b51d-a095bbdf810d\nspec:\n  capacity: \"5368709120\"\n  fsType: zfs\n  ownerNodeID: ix-truenas\n  poolName: NAS79_VOL_MIR_02/ix-applications/default_volumes\n  shared: \"yes\"\n  volumeType: DATASET\nstatus:\n  state: Ready\n</code></pre> On peut v\u00e9rifier le montage depuis le pod :</p> <pre><code>#sudo k3s kubectl -n ix-qbittorrent exec -it qbittorrent-6dbbbf9787-pl9q6 -- bash\n\nbittorrent-6dbbbf9787-pl9q6:/config$ df -h /config\nFilesystem                Size      Used Available Use% Mounted on\nNAS79_VOL_MIR_02/ix-applications/default_volumes/pvc-c58380a0-25d9-4080-a18f-e13b150edd13\n                          5.0G      5.6M      5.0G   0% /config\n</code></pre>","tags":["Kubernetes","TrueNAS","ZFS"]}]}